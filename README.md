# Udacity_AI_Scientist_Nanodegree_2nd_Project

This project is part of my Udacity program, where I built a transformer model from scratch to classify IMDB movie reviews as positive or negative.
I implemented a custom transformer architecture ("DemoGPT") and trained it on the IMDB dataset using subword tokenization (BERT tokenizer). 


## What I did
1- Loaded and processed the IMDB dataset
2- Used the BERT tokenizer for subword tokenization
3- Built a transformer model from scratch (attention heads, feed-forward layers, positional embeddings)
4- Trained it
5- Evaluated accuracy on validation and test sets

The final model reached 76.5% test accuracy, which was the goal. 


## What I Learned
1- How transformers actually work.
2- How much tokenization impacts model performance
3- How to build a full ML pipeline
4- How to debug training loops and get a model to converge
